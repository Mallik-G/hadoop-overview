# Setup Flume

So we first ssh!

All we need to do is write a config file that says what sources, sinks and channels are associated with each agend in our flume setup.

We can have a config file that states all.

Let's dwnload a simple one:

```
wget http://media.sundog-soft.com/hadoop/example.conf
```

here's the file

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
```

It starts by saying which are the sources sinks and channels.

The first thing is a setup a source that listens to telnet through tomcat.

The convension is to call sources = r, sinks = k and channels = c.

You can call them whatever you want tho.

Agent 1 = a1 = sources consist of r1, sinks = k1 and channels = c1

Sources a1.sources.r1.type = netcat, bind=localhost and port=4444

Our sink is just going to type = logger

our channels will be type = memory, capacity = 1000, capacity = 100. Options are normally memory or file.

There is extensive documentation in the website that explains the rest.

We bind the rest with: a1.sources.r1.channels = 1c and a1.sinks.k1.channel = c1.

Let's use it!

Let's open a new terminal and run flume from there

```
cd /usr/hdp/current/flume-server/
bin/flume-ng agent --conf conf --conf-file ~/example.conf --name a1 -Dflume.root.logger=INFO,console
```

It's kicked out the information that we setup.

let's now connect to it:

```
telnet localhost 44444

Hello there how are you today
Fourscore and seven years ago
```

It worked. We can see the bytes there.

Source in flume is listening to the data coming in from connection.

Outputting it to the log.

We can now quit.

Let's put some data in there...

```
cp access_log_small.txt spool/fred.txt
```

We can see something happened, and it suffixed a .COMPLETED to the file. So now its possible to delete it.

If we go to HDFS we can see the file in the format we specified.

We can see the raw log data and we can see that it's easily importaed by our tools in our cluster.

This is closer to a real world example.

We can exit out of everything.










