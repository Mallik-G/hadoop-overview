
# Integrating MySQL & Hadoop

Integrating RBDs! Scoop!

# What was SQL?

* Popular and free relational database
* Generally monolithic in nature
* But, can be used for OLTP (online transactional processing) - 
    - So exporting data into mySQL can be useful
* Existing data might exist that you want to extract


# Sqoop can handle big data

Actually kicks off MR jobs to handle importing or exporting your RBD data!

MySql/Postgres/Whatever -> Mapper|Mapper|Mapper -> HDFS

HDFS is distributed across cluster and tries to keep stuff locally, mappers might be in the same hosts to improve quality. 

As any map reduce jobs, there will be some time calculating how to export/import.

## Sqoop : import data from MySQL to HDFS:

```
sqoop import --connect jdbc:mysql://DB_HOST/DB_NAME --drive com.mysql.jdbc.Driver --table movies
```

Assuming we have proper permissions, then this command would take all of the movie tables into the HDFS as a bunch of flat files.

You can also provide how many mappers we want to use. When there's only 1 server, there's not much point to use more than 1 mapper.

If you just want to use data in HIVE, you just have to add the line at the bottom of the query. So it's quite easy on how to run it.

# Incremental Imports

* You can keep your relational database and Hadoop in sync
* --check-column and --last-value

This lets you do stuff like, import the database, and then put the last value here, and it will only start from there. Keeping your hive in sync.

# Sqoop: Export data from hive to mysql

```
sqoop export --connect jdbc:mysql://MY_HOST/DB_NAME -m 1 --driver com.mysql.jdbc.Driver --table DB_TABLE --export-dir /apps/hive/warehouses/movies --input-fields-terminated-by '\00001'

# -m 1 # means 1 mapper
```

# Let's play with MySql and Sqoop

* Import MovieLens data into a MySql database
* import movies to HDFS
* Import the movies into Hive
* Export the movies back into MySQL



   
