
# SPARK SQL
## Spark2.0 way of DataFrames and DataSets

# Working with structure data

* RDDs have no stuff, they are just blobs of data
* Dataframes extend RDD to "DataFrame" object
* DataFrames:
    - Contain row objects
    - Can run SQL queries
    - Has a schem (leading to more efficient storage)
    - Read to write to JSON, Hive, Parquet
    - Communicates with JDBC/ODBC, Tableau

# SparkSQL in Python

* from pyspark.sql import SQLContext, Row # Just import this and should work
* hiveContext = HiveContext(sc)
* inputData = spark.read.json(dataFile)
* inputData.createOrReplaceTempView("MyStructuredStuff")
* myResultDataFrame = hiveContext.sql("""SELECT foo FROM bar ORDER by foobar)

# Other stuff you can do with dataframes

* myResultDataFrame.show()
* myResultDataFrame.select("someFieldName")
* myResultDataFrame.filter(myResultDataFrame("someFIeldName">200))
* myResultDataFrame.groupBy(myResultDataFrame("somefied")).mean()
* myResultDataFrame.rdd().map(mapperFunction)

# Datasets

This is a new concept

* In spakr 2.0, a dataframe is really a dataset of row objects
    - Can give you more compile time erros instead of waiting til runtime
* Dataset can wrap known, typed data too. But this is mostly transparent to you in Python, since Python is dynamically typed.
* So - don't sweat this too much with Python. But the Spark 2.0 way is to use Datasets instead of DataFrames when you can

# Shell access

* Spark SQL exposes a JDBC/ODBC server (if built spark with hive support
* Start with sbin/start-thriftserver.sh
* Listens on port 10000 by default
* Connect using bin/beeline -u jdbc:hive2://localhost:10000
* Viola, you have a SQL shell to Spark SQL
* You can create new tables or query existing ones that were cached using hiveCtx.cacheTable("tableName")

# User-defined Functions (UDF's)

* from pyspark.sql.types import IntegerType
* hiveCtx.registerFunction("square", lambda x: x*x, IntegerType())
* df = hiveCtx.sql("SELECT square('someNumericFiled') FROM tableName")






