
# Introducing RDDs

* Resillient
    - Should be fault tolerant
* Distributed
    - Evenly dist across cluster
    - Resilient
* Datasets
    - Should look like a dataset to use

RDD = just a dataset to you, but it's resillient and distributed in the background.


# The SparkContext

* RDDs created by your driver program
* Is responsible for making RDD's resillient and distributed
* Creates RDD's
* The Spark shell creates a "sc" object for you

The environment that the program runs

# Creating an RDD

* You can create it from given list fo values
    - nums = parallelize([1,2,3,4])
    - You can do stuff with RDD and distribute it to an entire cluster
    - If I can fit all memory in my script you're not talking about big data
* sc.textFile(...)
    - More common
    - You can refer from hdfs:// or s3n:// (AWS S3 URLs)
* hiveCtx = HiveContext(sc) rows = hiveCtxt.sql("SELECT name, age FROM users")
    - You can just use the Hive context and then run queries
* Can also get from
    - JDBC
    - Cassandra
    - HBase
    - Elasticsearch
    - JSON, CSV, sequence files, object files, various compressed formats

# Transformaing RDDs

* Map
    - 1-1 relationship, when input->outpu
* Flatmap
    - Map is good bug sSOmetimes you want some rows into 1 row or the other way
    - Might or might not result in a 1-1 relationship of input/output
* Filter
    - take stuff out from RDD
    - Whether a row in RDD survives
* Distinct
    - Removes repeated
* Sample
    - Sample randomly
* UNion, intersection, subtract, cartesian

# MAP Example

Let's take integers and square them!

* rdd = sc.parallelize([1,2,3,4])
* squaredRDD = rdd.map(lambda x: x*x)

* This yields 1,4,9,16

# What's that Lambda Thing?

It's the syntax for the functional programming paradigm

# RDD Actions

If you want to crunch numbers down

* collect
    - Take all in RDD, and return python object to play with
    - Print, save to text, etc
* count
    - How many rows in rdd
* countByVal
    - How many per value
* take
    - Returns few rows of RDD
* top
    - Returns top few rows
* reduce
    - Define function that combines all values to a unique key
* and more...

# Lazy evaluation

Nothing gets actioned until you call a RDD Action function
That's when it runs backwards and tries to find the result required.







