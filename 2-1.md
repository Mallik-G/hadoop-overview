
# HDFS: What is it and how it works

The hadoop distributed file systems.

# Overview

* Handles large files
    - Can be logs with info from sensors, etc
* Does this by breaking into blocks
    - Blocks are quite large - 128megs by default (big files)
    - If less than that goes to only 1 block
    - No limitations for single hard drive
    - Also allows to distribute processing of the large files
        + Different computers can process a chunk of data
* Blocks can be stored across commodity computer
    - Store more than one copy of each block
    - If computers go down, its' possible to use backups
    - If single block goes down, we don't lose info
    - This is the power of HDFS
    - Don't need special computers with high availability as if it doesn't work we can use another node

## HDFS Architecture

* **Name node** There is a name node: file name on virtual dir structure
    - It knows where to go 
    - There is an EDIT LOG to keep track where everything is
* **Data node** - What actually stores each block of each file
    - Nodes talk to each other

## Example: Reading a file

Client node needs to access data in HDFS.

* Talk to node
    - Node comes back and says which nodes are the data
    - Then we have to go to the specific nodes to retrieve the data
    - Under the hood the HDFS figures out where to get, how to get, etc
    - Also it knows which ones are the most efficient

## Example: Writing a file

* Client wants to create a new file
    - Name node keeps track of new entries and its blocks
    - Then creates an entry, then client talks to data node with file/data
    - Then nodes talk to each other to replicate the data across nodes
    - Once data is stored, there is an acknowledgement sent back

 