
# HDFS: What is it and how it works

The hadoop distributed file systems.

# Overview

* Handles large files
    - Can be logs with info from sensors, etc
* Does this by breaking into blocks
    - Blocks are quite large - 128megs by default (big files)
    - If less than that goes to only 1 block
    - No limitations for single hard drive
    - Also allows to distribute processing of the large files
        + Different computers can process a chunk of data
* Blocks can be stored across commodity computer
    - Store more than one copy of each block
    - If computers go down, its' possible to use backups
    - If single block goes down, we don't lose info
    - This is the power of HDFS
    - Don't need special computers with high availability as if it doesn't work we can use another node

## HDFS Architecture

* **Name node** There is a name node: file name on virtual dir structure
    - It knows where to go 
    - There is an EDIT LOG to keep track where everything is
* **Data node** - What actually stores each block of each file
    - Nodes talk to each other

## Example: Reading a file

Client node needs to access data in HDFS.

* Talk to node
    - Node comes back and says which nodes are the data
    - Then we have to go to the specific nodes to retrieve the data
    - Under the hood the HDFS figures out where to get, how to get, etc
    - Also it knows which ones are the most efficient

## Example: Writing a file

* Client wants to create a new file
    - Name node keeps track of new entries and its blocks
    - Then creates an entry, then client talks to data node with file/data
    - Then nodes talk to each other to replicate the data across nodes
    - Once data is stored, there is an acknowledgement sent back
    
## Namenode resilience

* Backup metadata
    - Namenode writes to local disk and NFS
    - It's possible to re-boot from this
    - There will be some lag with writing the data
    - If you can tolerate downtime, this is the simplest way
* Secondary Namenode
    - It's not a hot backup
    - It's a merged copy of edit log you can restore from
    - Little bit better than previous solution
* HDFS Federation
    - Each namenode manages a specific namespace volume
    - You might have a lot of small files, and namenodes can't fit more data
    - HDFS federation manages multiple namespaces for a specific volume
    - You can spread out the namenodes
    - If one of the namenodes goes down, you don't loose all the data
* HDFS High Availability
    - If you can't stand any downtime
    - Hot standby namenode using shared edit log
    - Writing to file system that is not HDFS
    - Backup can take up
    - Uses Zookeeper to keep track which node is active at any given time
    - Uses extreme measures to ensure only one namenode is used at a time

# Using HDFS

* Can use UI (Ambari)
    - Just looks like a big hard drive (across loads of servers, etc)
    - Command Line Interface
    - HTTP/HDFS Proxies
    - Java interface 
    - NFS Gateway - Mounting a remote file system





