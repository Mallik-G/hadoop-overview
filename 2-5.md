
# MapReduce on a Cluster - How MR Scales

Now we'll talk MapReduce in an under-the-hood perspective.

IN the previous lesson we ran MR with the movie ratings.

We produced a MAPPER, SHUFFLE AND SORT and a REDUCER.

We might have multiple computers, nodes, depending of how complex the query is.

We'll see how it would look like if this gets broken down in 3 nodes.

Dataset might get chopped into 3 parts - and will send the data to 3 nodes for processing (first few lines to first, etc)

Carve up input data to partitions and assign partitions to the nodes.

It's easy to parallelize the data.

Hadoop only has to keep track when it's all done.

You might also have some shuffle and sort operations that can spread the information of the mappers into less nodes, making it a bit more complex

But this is all handled by the hadoop system.

Reducers also can be run in parallel, reducing specific sets of keys.

Result gets streamed back to the client node.

# Anatomy of MR 

* Client node triggers the job
* Client node talks to YARN - Yet Another Resource Negotiator 
    - Keeps track which nodes are available, which have capacity, etc
    - Talks to resource manager saying to kick off resource manager
    - Whilst doing that, it also talks to HDFS (or any data system) to copy any data needed
* Then Map Reduce application master is spawned up called **NODE MANAGER**
    - Keeps track whether node is doing, etc
    - App master keeps track of MR trasks, and works with Resurce Manager to distribute work across cluster
    - Whilst these nodes are mapping and reducing, it's talking to the HDFS clusters to get the data, and output the data when done

When resource manager decides where to actually launch a MR, it will try to run it as close to the data as possible.

Input data will be split into blocks across clusters - so a mapper responsible to that should be as close as possible (even in the same server). It tries not to send stuff across the network when it doesn't need.

You can have multiple tasks running in the same physical computer.

# How are mappers and reducers written?

* MapReduce is natively Java
* STREAMING allows interfacing to other languages (ie Python)
    - Idea is that a MapTask can be kicked off through STDIN & STDOUT
    - Instead of mapping funcitons on java
        + You can run them with anything with STDIN / STDOUT
    - Mapper that wants to run would send input data in STDIN, and output key values in STDOUT

# Handling Failure

* Application master monitors worker tasks for errors or hanging
    - Restarts as needed
    - Preferably on a different node
* What if the application master goes down?
    - YARN can try to restart it
* What if an entire Node goes down?
    - This could be the application master
    - The resource manager will try to restart it
* What if the resource manager goes down?
    - Can set up "high availability" (HA) using Zookeeper to have a hot standby















