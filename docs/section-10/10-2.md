# Spark Streaming Example

Let's start by logging in to VM!

Configuring flume to talk to spark streaming.

Before we made flume direct the data to HDFS. 

Now we'll make it direct to avro, and then will connect to spark streaming

Lets get the file

```
wget media.sundog-soft.com/hadoop/sparkstreamingflume.conf
```

Everything is the same, the only difference is tha tthe sink type is avro.

```
# Describe the sink
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = localhost
a1.sinks.k1.port = 9092
```

Now let's take a look at the sparkstreaming 

```
wget media.sundog-soft.com/hadoop/SparkFlume.py
```

Now we have the contents of the script. There is a bit of housekeeping.

* Starts with a regex on how to extract stuff from apache log data
    - Extracting host, ident, user, time, request, status, size, referrer, user
* NEx thing is our map function!
    - extractURLRequest(line)
        + Takes our regex, and tries to find a match, and the group it
        + Split into compoennts, and url gets extracted
* MAIN
    - Setup SParkContext!
    - ```setLogLevel("ERROR")```
    - Then set streaming context ```sc=StreamingCOntext(sc, 1 second batch)```
    - Now to hook it to flume, we use ```fs = FlumeUtils.createStream```
        + This is only 1 of 2 ways 
        + This is a push model from flume to avro
        + You can also setup a poll model
            * Cusotm sync to establish a bidirectional relationshi
            * More robust way
            * More complicated as well
    - We use a map opertation to extract the line ```lines=fs.map(l x: x[1])```
    - And we map on the ```urls = lines.map(extractURLRequest)```
    - And the fun begins - we reduce URL over 5 minute window sliding 1 sec
        + urls
            * We map each url to pair 1  - .map(l x: (x, 1))
            * THen reduce on window - .reduceByKeyAndWindow(l x,y:x+y,l x,y:x-y, 300, 1)
                - For each scond, take 5 min window
    - THen finally get the results
        + sortedResults = urlCounts.transform(l rdd: rdd.sortBy(l x: x[1], False))
    - Now we want to maintain a checkpoint directory
        + When you do a window operation it needs to put state somewhere
        + If cluster goes down we can start where it left off based on checkpoint
    - After that we just start the job!

Now let's run it!

``` 
# We need to make checkpoint dir
cd ~
mkdir checkpoint

export SPARK_MAJOR_VERSION=2

spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py
```

Since flume is not running we're not gonna see anything, but let's kick off flume!


in new terminal:

```
cd /usr/hdp/current/flume-server/

bin/flume-ng agent --conf conf --conf-file ~/sparkstreamingflume.conf --name a1
```

Now we'll need a 3rd window!

```
wget http://media.sundog-soft.com/hadoop/access_log.txt

cp access_log.txt spool/log22.txt
```




