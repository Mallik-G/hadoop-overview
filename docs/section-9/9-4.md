# Apache Flume

* Another way to stream data into your cluster
* Made from the start with hadoop in mind
    - Build in sinks for HDaFS and HBase
* Originally made to handle log aggregation

Why can't I just directly transfer data to HDFS? Log traffic can be very spikey. By introducing Flume in the middle, you create a middleware that won't bring down your cluster and gives everything to catch up.

# Anatomy of Flume Agent and Flow

When we talk about flume we talk about flume agent

I want to setup some agents, sources and sinks.

* We might have a flume source as part of agent that listents to web servers
* It can then go to a channel
* which talks to a sinka
* And then talks to HBase or HDFS

# Components of an agent

* Source
    - Where data is coming from 
    - Can optionally have channel selectors and interceptors
* Channel
    - How the data is transferred (via memory or files)
* Sink
    - Where the data is going
    - Can be organized into Sink Groups
    - A sink can connect to only one channel
        + Channel is notified to delete a message once the sink processes it

# Built-in Source Types

Out of the box

* Spooling directory
    - Can monitor directory and new files, etc
* Avro
    - A format communication to hadoop
* Kafka
    - Can conncet to kafka
* Exec
    - Command in linux (tail -f)
* Thrift
    - Another data connection interface
* Netcat
    - listen to tcp port
* HTTP
    - Web traffic
* Custom
    - You can write custom ones 
* MORE!

# built in sink types

* HDFS
    - Write directly into hadoop clustre
* Hive
* HBase
* Avro
    - Can use as data protocol to connect other together
* Thrift
* ElasticSearch
* Kafka
* Custom
* MOAR!!

# Using Avro, agents can connect to other agents as well

* powerful things about flume is that you can have multi-tier designs
* A bunch of app servers can go to a few agents, generating a lot of network traffic
* But then  you can have another flume layer that consolidates into a tier that has less agents
* And final layer can just write to HDFS

This is often much more scalable. If you have multiple datacentres, this would work well as well.

The way this works is that the first Tier flume agents are set to talk directly to the app servers and monitor the files directly. 

For the agents talking to other agents you would have sinks and sources to make them connect with each other.

Then the last layer would have a sink as well.

# Think of flume as buffer between your data and cluster

If you have HDFS backing uup , then if flume agent starts buffering up, then the upper tiers start buffering up as well. SO you have multiple layers.

You want to design your flume flows to avoid hitting limit of all layers - giving reasonable length of outages


# Let's Play: Simple flow

* Source: netcat
* Channel: memory
* Sink: Logger

We'll setup a flow wiht a single agent wiht a source of netcat which only listents via tcp.

We type data and it gets imported to flume

The channel does nothng else but just channel it to the sink which is a logger

# 2nd example

Later we'll do more interesting stuff

* Files into a directory
* Source: Spooldir that listens to new files
    - Timestamp inteceptor of when data was dropped
* Channel: memory
* Sink: HDFS
    - And then we put it in a directory hieararchy where data will be stored in folders based on timestamps.

 











