# Streaming!

Publishing data from source like web blogs, sensor data, published from a scalable manner.

How you go about publishing data from sources into db in real time

# Streaming with Kafka

Puublic/subscribe messaging system

## What is streaming?

We're assuming that data is in our cluster, but how does the data comes to the cluster?

Sometimes we want to process data in real time

* So far we've really just talked about processing historical, existing big data
    - Sitting on HDFS 
    - Sitting in DB
* But how does new data get into your cluster? Especially if it's big data?
    - New log entries from your web servers
    - New sensor data from your IoT system
    - New stock trades
* Streaming lets you publish this data, in real time, to this cluster
    - And you can even process it in real time as it comes in

# Two problems

* How to get data from many sources flowing into your cluster
* Processing it when it gets there

First start with the first part!

# Enter Kafka

* Kafak is a general purpose publish/subscribe messaging system
* Kafka servers store all incoming messages from publishers for some period of time, and publishes them to a stream of data called a topic
* Kafka consumers subscribe to one or more topics, and receive data as it's published
* A stream/topic can have many different consumers, all with their own position in the stream maintained
* It's not just for Hadoop

## Kafka Architecture

Architecturally, you can think of a kafka cluster at the center of a producer network.

**Producers** might be individual apps listening for data. They communicate with the thing generating the data and kafka.

**Consumers** subscribe to the topics and receiving to that daata. Usually there are existing apps that come out of the shelf. Kafka might come with built in ones.

**Connectors** - modules for dbs, plugins that allow it to receive new messages and store, or actually publish changes into Kafka itself.  You can monitor an existing database, and process it as it comes in.

**Stream Processors** transform data as it comes in. Producer might be producing unstructured web logs, and you might have stream processors that process the data and re-publishes that as a new topic into Kafka.

# How kafka scales?

* Kafka itself may be distributed among many processes on many servers
    - Will distributed the storage of stream data as well
* Consumersmay also be distributed
    - Consumers of the same group will have messages distributed amongst them
    - Consumers of different groups will get their own copy of the message

# Time to play!

* Start kafka on our sandbox
* Setup topic
* Publish data into it, and wwatch it get consumed
* Setup a file connector
* Monitor a log file and publish addiions to it 





