# Playing with Zepplin

To kick off zepplin, we only have to go to 127.0.0.1:9995!

And that's our notebook!

Let's figure out what Zepplin is all about. It's a notebook interface to our cluster.

Create a new notebook called "Playing with MovieLens"

Now we have a blank notebook we can do stuff with!

We can now run spark code with!

This has multiple interpreters, so we can ask to prioritise by clicking on the gear at the top right. Spark should be at the top, md second.

It's a good tool for experimenting in your cluster, and documenting stuff, etc.

We can write:

```
%md
### Let's make sure Spark is working first!
Let's see what version we're working with
```

Now that we've said what we're gonna do let's do it!

Spark is our primary backend, so we can start writing and see what happens!

Spark has a context object called sc, and a spark sql called sq.

```
sc.version
```

We're gonna use the scala programming language.

We can execute shell commands from these.

```
%sh
wget http://media.sundog-soft.com/hadoop/ml-100k/u.data -O /tmp/u.data
wget http://media-sundog-soft.com/hadoop/ml-100k/u.item -O /tmp/u.item
echo "Downloaded!"
```

Now...

```
%sh
hadoop fs -rm -r -f /tmp /ml-100k

hadoop fs -mkdir /tmp/ml-100k

hadoop fs -put /tmp/u.data /tmp/ml-100k
hadoop fs -put /tmp/u.item /tmp/ml-100k
```

You can also give titles to the codes!

Now let's write some scala code

``` scala
final case class Rating(movieID: Int, rating: Int)

val lines = sc.textFile("hdfs:///tmp/ml-100k/u.data").map(x => {val fields = x.split("\t"); Rating(fields(1).toInt, fields(2).toInt) })
```

Let's not create a dataframe!

``` scala
import sqlContext.implicits._
val ratingsDF = lines.toDF()

ratingsDF.printSchema()
```

and to find the movies

``` scala
val topMovieIDs = ratingsDF.groupBy("movieID").count().orderBy(desc("count")).cache()
topMovieIDs.show()
```









