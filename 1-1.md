
# Introduction, Installation

## About Instructor

Worked 9 years in Amazon + IMDB

## Objective of course

* Understand all of the big data products.
* Writing programs with Python & Scala
* Exporting / importing data from relational databases

# Installation

* First thing: Install VM, so go to [virtualbox.com](https://virtualbox.com)
* We will download & install VirtualBox for whichever OS you have
* Then we need image for Hadoop at: [hortonworks.com/sandbox](https://hortonworks.com/sandbox)
* When downloaded, we open the file and load it to VirtualBox

We downloaded a pre-installed hadoop environment.

Now we can just start the VM!

# Download data

Now let's download data!

Let's download the MovieLens 100k Dataset from [grouplens.org](https://grouplens.org) 

We download and decompress the ml-100k.zip.

The **u.data** is what contains all the data.

* 1st col is user ID
* 2nd Col is movie ID
* 3rd Col is rating (out of 3 stars)
* Then timestamp

If we want to know what the movie ID is, then we can check it in the **u.item** file


# Navigating 

Once it loads up we'll have a CentOS machine running. It has a browser interface that allows us to see what's going on in our hadoop instance.

We can go to [localhost:8888](http://localhost:8888), and we'll see a screen to log in.

* Username: maria_dev
* Password maria_dev

We see our Ambari dashboard to see what's going on.

With hadoop you don't have to worry on how many nodes of clusters it's running as it's abstracted.

# Importing the data 

We have u.data and u.item which we want to import.

We can do it through the visual interface.

We open the **HIVE tool**. It will load everything, then click in **upload table** - we now upload the files with the respective delimiters.

The table name is **ratings**.

Columns are **user_id, movie_id, rating and rating_time**, together with the relevant types (INTS).

Tables will now be created!

We now repeat with u.item (which is delimitered by pipes) and call this table **movie_names**.

Columns are just **movie_id and name** - the rest will be left as default ("column#")

# Handling the data with HIVE

We can now start making calculations! Using standard SQL queries...

Example query:

``` SQL
    SELECT movies_id, count(movie_id) as ratingCount
    FROM ratings
    GROUP BY movie_id
    ORDER BY ratingCount
    DESC
```

This should give us a sorted list of the most popular to the least popular database - it's now doing a map reduce cluster

Hive view also has a visualisation option... We can see distributions for say movie_id and ratingcount.

Let's find out which is movie 50 (as that was the most popular)

``` SQL
    SELECT name
    FROM movie_names
    WHERE movie_id = 50;
```

The answer is... **STAR WARS**!





