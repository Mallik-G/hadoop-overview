
# Introduction, Installation

## About Instructor

Worked 9 years in Amazon + IMDB

## Objective of course

* Understand all of the big data products.
* Writing programs with Python & Scala
* Exporting / importing data from relational databases

# Installation

* First thing: Install VM, so go to [virtualbox.com](https://virtualbox.com)
* We will download & install VirtualBox for whichever OS you have
* Then we need image for Hadoop at: [hortonworks.com/sandbox](https://hortonworks.com/sandbox)
* When downloaded, we open the file and load it to VirtualBox

We downloaded a pre-installed hadoop environment.

Now we can just start the VM!

# Download data

Now let's download data!

Let's download the MovieLens 100k Dataset from [grouplens.org](https://grouplens.org) 

We download and decompress the ml-100k.zip.

The **u.data** is what contains all the data.

* 1st col is user ID
* 2nd Col is movie ID
* 3rd Col is rating (out of 3 stars)
* Then timestamp

If we want to know what the movie ID is, then we can check it in the **u.item** file


# Navigating 

Once it loads up we'll have a CentOS machine running. It has a browser interface that allows us to see what's going on in our hadoop instance.

We can go to [localhost:8888](http://localhost:8888), and we'll see a screen to log in.

* Username: maria_dev
* Password maria_dev

We see our Ambari dashboard to see what's going on.

With hadoop you don't have to worry on how many nodes of clusters it's running as it's abstracted.

# Importing the data 

We have u.data and u.item




