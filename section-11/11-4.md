# Example Top Sellers

## What we want to build?

This was a question that the instructor asked in his interviews to see if applicants can wrap their heads about all the systems.

* A system to track and display the top 10 best-selling items on an e-commerce website

What differentiated the REALLY good candidates from the good candidates were the ones that asked, **"Well, what does it mean to be best selling?"**.

And over what period of time? Is it the 10 best items for the past hour? Past, etc?

Once you think of those quesitons, you'll start think in the right time.

# What are our requirements? 

**Work backwards**

* There are millions of end-users, generating thousands of queries per second
    - It MUST be fast - page latency is important
    - So, we need some distributed NoSQL solution
    - Access pattern is simple: "Give me the current top N sellers in category X"
* Hourly updates probably good enough (consistency not hugely important)
* Must be highly available (customers don't like broken websites)
* So - we want partition-tolerance and availability more than consistency

**SOUNDS LIKE CASSANDRA!**

Amazon DynamoDB would be a similar...

Most databases are quite similar tbh tho... 

It's not going to be the end of the world if you already have an internal MongoDB team...

# But how does data get into cassandra?

* Spark can talk to Cassandra...
* And Spark Streaming can add things up over windows

# How does data get into spark streaming?

* Kafka or Flume - either works
* Flume is purpose-built for HDFS, which so far we haven't said we need
* But flume is also purpose build for log ingestion, so it may be a good choice
    - Log4j interceptor on the servers that process purchases?

# Don't forget about security

* Purchase data is sensitive - get a security review
    - Blasting around raw logs that include PII* is probably a really bad idea
    - **Strip out data your don't need at the source**
* Security considerations may even force you into a totally different design
    - Instead of ingesting logs as they are generated, some intermediate database publisher may be involved where PII is scrubbed

# So, something like this might work:

**Purchase servers -> Zookeeper/Flume -> Spark Streaming -> Cassandra -> WebServ**

Each of these sections are an entire cluster by itself

# but there's more than 1 way

* Mayeb you have an existing purchase database
    - Instead of streaming, hourly batch jobs would also meet your requirements
    - Use Sqoop + Spark -> Cassandra Perhaps?
* Maybe you have in-house expertise to leverage
    - Using HBase, MongoDB, or even Redis instead of Cassandra would probably be ok
    - Using Kafka instead of Flume - totally OK
* Do people need this data for analytical purposes too?
    - Might consider storing on HDFS in addition to Cassandra


