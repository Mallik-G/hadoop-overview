# Process data streams

How to process data as it comes

Apache straming / storm. How to stream and process

# Spark STREAMING

## Why Spark Streaming?

* Big data never stops coming in
* Analyse data streams in real time, instead of in huge batch jobs daily
    - What happens if batch gets too big
    - You can do it as it comes
* Analyzing streams of web log data to react to user behaviour
* Analyze streams of real time sensor data for "Internet of Things" stuff

## High level architecture

* You have bunch of streams coming in
    - Can be kafka, etc
* Spark cluster has receivers across cluster
    - Discretizes data into little chunks
* You can give increment chunks into individual RDDs
    - Contains discritized chunk of data received in a window of  time specified
* You can transform & output to other systems

It's not 100% real time, as it's batches, but you're breaking them at 1s chunks so i'ts kinda real data.

# Work can be distributed

Processing of RDDs can happen in parallel on different worker nodes across spakr/hadoop cluster


# DStreams (Discretized Streams)

* Generates the RDDs for each time step
    - Can produce output at each time step
* Can be transformed and acted on in much the same way as RDDs
* Or you can access their underlying RDDs if you need them

# Common stateless transformations on DStreams

* Map
* Flatmap
* Filter
* ReduceByKey

All powerful things you can do in spark RDD you can do in DStream

# Stateful Data

* You can also maintain a long-lived state on a DStream
* For example - running totals, broken down by keys
* Another example: Aggregating session data in web activity

Not limited to dealing just microbatches - you can maintain state

# Windowing

That's how state works

* Allow you to compute results across a onger time period than your batch interval
* Example: Top-sellers from the past hour
    - You might process data every one second (the batch interfal)
    - But maintain a window of one hour
* The window "slides" as time goes on, to represent batches within the window interval

# Batch interval vs Slide interval vs Window interval

* the batch interval is how often data is captured into a DStream
* The slide interval is how often a windowed transformation is computed
* The window interval is how far back in the time the windowed tranformation goes

# Example

* Each batch contains one second of data (the batch interval)
* We set up a window interval of 3 seconds and a slide interval of 2 seconds
    - Every second we generate info
    - Every 2 seconds I would compute a result
    - Window interval, I want to look 3 seconds back

 Spark streaming is a complicated topic.

 # Windowed transformations code

* The batch inteval is set up with your SparkContext:
    - ssc = StreamingContext(sc, 1)
* You can use reduceByWindow() or reduceByKeyAndWindow() to aggregate data across a longer period of time!
    - hashtagCounts = hashtagKeyValues.reduceByKeyAndWindow(lambda x, y: x+y, lambda x, y: x - y, 300, 1 )


# Structured Streaming

## What is structured streaming?

* A new, higher-level API for streaming structured data
    - Available in Spark 2.0 and 2.1 as an experimental release
    - But it's the future
* Uses DataSets
    - Like a dataframe, but with more explicit type information
    - A DataFrame is really a DataSet[Row]

## Imagine a DataFrame that never ends

* New data just keeps getting appended to it
* Your continuous application keeps queying updated data as it comes in

## Advantages

* Streaming code looks a lot like the equivalent non-streaming code
* Structured data allows Spark to represent data more efficiently
* SQL-style queries allow for query optimization opportunities - and even better performance
* Interoperability with other Spark components based on Datasets
    - MLlib also moving towards DataSets as its primary API
* DataSets in general is the direction Spark is moving

## SparkSession

Say we want to monitor an S3 bucket for new log data and group up everything in 1 hour window, we can do it in very little code.

Once you have a sparksession, you can stream data, query it and write out the results

2 lines of code to stream in structured JSON log data, count up "action" values for each hour and write the results to a database

``` scala
var inputDF = spark.readStream.json("s3://logs")
inputDF.groupBy($"action", window($"time", "1 hour")).count()
    .writeStream.format("jdbc").start("jdbc:mysql//...")
```

Structured streaming is still experimental with Spark, so we'll stick with normal spark streamting

# Example: Spark Streaming with flume

* We'll set up flume to use a spooldir source as before
* But use an Avro sink to connect it to our Spark Streaming job
    - Use a window to aggregate how often each unique URL appears from our access log
* Using avro in this manner is a push mechanism to Spark streaming
    - You can also "pull" data by using a custom sink for Spark streaming

**Logs->Source(spooldir)->Channel(memory)->Sink(Avro)->SparkStreaming->Console**

Spark streaming runs in its own cluster, as well as flume. You can scale the processing of pushing the data to the processing of the data itself.








